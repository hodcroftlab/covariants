# Run this script from within an 'ncov' directory ()
# which is a sister directory to 'covariants'
# See the 'WHERE FILES WRITE OUT' below to see options on modifying file paths
# Importantly, ensure you create a real or fake 'ncov_cluster' output directory - or change it!

# TLDR: make sure 'ncov' and 'covariants' repos are in same directory
# 'ncov_cluster' should also be there - or create empty folder to match paths below

######### INPUT FILES
# This requires two files that cannot be distributed publicly:
# ncov/data/meatdata.tsv (can be downloaded from GISAID as 'nextmeta')
# ncov/results/sequence-diagnostics.tsv
#    this file unfortunately isn't available but can be generated by running the 'ncov' pipeline.

# For Nextstrain members only:
#       You can get these by downloading the most recent run from AWS
#       (see slack #ncov-gisaid-updates for the command)
#       Or by running an `ncov`` build locally/on cluster until sequence-diagnostics.tsv is generated

######### WHERE FILES WRITE OUT
# If you want to output files to run in `ncov_cluster` to make cluster-focused builds,
# clone this repo so it sits 'next' to ncov: https://github.com/emmahodcroft/ncov_cluster
# and use these paths:
cluster_path = "../ncov_cluster/cluster_profile/"

# Things that write out to cluster_scripts repo (images mostly), use this path:
tables_path = "../covariants/cluster_tables/"
overall_tables_file = "../covariants/cluster_tables/all_tables.tsv"
acknowledgement_folder = "../covariants/acknowledgements/"
acknowledgement_folder_new = "../covariants/web/public/acknowledgements/"
web_data_folder = "../covariants/web/data/"
# This assumes that `covariants` sites next to `ncov`
# Otherwise, modify the paths above to put the files wherever you like.
# (Alternatively just create a folder structure to mirror the above)

fmt = "png"  # "pdf"
grey_color = "#cccccc"  # for "other clusters" of country plots

#dated_limit = "2021-03-31" #only works for Q677 currently
#dated_limit = "2021-06-30"
#dated_cluster = "21A (Delta)"
#dated_cluster = "20I (Alpha, V1)"
dated_cluster = "Q677"
dated_limit = ""

import pandas as pd
import datetime
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from shutil import copyfile
from collections import defaultdict
from matplotlib.patches import Rectangle
import json
import matplotlib.patches as mpatches
from colors_and_countries import *
from helpers import *
from clusters import *
from bad_sequences import *
from approx_first_dates import *
from swiss_regions import *
import os
import re
import time
import sys

def get_division_summary(cluster_meta, chosen_country):

    country_meta = cluster_meta[
        cluster_meta["country"].apply(lambda x: x == chosen_country)
    ]
    #remove any 'empty' divisions as we don't want these.
    observed_divisions = [x for x in country_meta["division"].unique() if x]

    division_info = pd.DataFrame(
        index=observed_divisions, columns=["first_seq", "num_seqs", "last_seq"]
    )
    division_dates = {}

    for div in observed_divisions:
        temp_meta = cluster_meta[cluster_meta["division"].apply(lambda x: x == div)]
        division_info.loc[div].first_seq = temp_meta["date"].min()
        division_info.loc[div].last_seq = temp_meta["date"].max()
        division_info.loc[div].num_seqs = len(temp_meta)
        division_dates[div] = [
            datetime.datetime.strptime(dat, "%Y-%m-%d") for dat in temp_meta["date"]
        ]

    division_info_df = pd.DataFrame(data=division_info)

    print("\nOrdered list by first_seq date:")
    print(division_info_df.sort_values(by="first_seq"))


def get_summary(cluster_meta, observed_countries, division=False):

    country_info = pd.DataFrame(
        index=observed_countries, columns=["first_seq", "num_seqs", "last_seq"]
    )
    country_dates = {}

    for coun in observed_countries:
        if coun in uk_countries or division:
            # temp_meta = cluster_meta[cluster_meta['division'].isin([coun])]
            temp_meta = cluster_meta[
                cluster_meta["division"].apply(lambda x: x == coun)
            ]
        else:
            # temp_meta = cluster_meta[cluster_meta['country'].isin([coun])]
            temp_meta = cluster_meta[cluster_meta["country"].apply(lambda x: x == coun)]

        country_info.loc[coun].first_seq = temp_meta["date"].min()
        country_info.loc[coun].last_seq = temp_meta["date"].max()
        country_info.loc[coun].num_seqs = len(temp_meta)

        country_dates[coun] = [
            datetime.datetime.strptime(dat, "%Y-%m-%d") for dat in temp_meta["date"]
        ]

    return country_info, country_dates

def print_date_alerts(clus):
    print(clus)
    print(f"Expected date: {cluster_first_dates[clus]['first_date']}")
    print(alert_first_date[clus][['strain','date','gisaid_epi_isl']])

def print_all_date_alerts():
    for clus in alert_first_date.keys():
        print_date_alerts(clus)
        print("\n")

def print_date_alerts_quick(clus):
    print(clus)
    print(f"Expected date: {cluster_first_dates[clus]['first_date']}")
    print(alert_first_date_quick[clus][['strain','date','gisaid_epi_isl']])

def print_all_date_alerts_quick():
    for clus in alert_first_date_quick.keys():
        print_date_alerts_quick(clus)
        print("\n")


def marker_size(n):
    if n > 100:
        return 150
    elif n > 30:
        return 100
    elif n > 10:
        return 70
    elif n > 3:
        return 50
    elif n > 1:
        return 20
    else:
        return 5


# Store first date alarms
alert_first_date = {}

# store acknoweledgements
acknowledgement_by_variant = {}
acknowledgement_by_variant["acknowledgements"] = {}

acknowledgement_keys = {}
acknowledgement_keys["acknowledgements"] = {}

# set min data week to consider
min_data_week = (2020, 18)  # 20)

##################################
##################################
#### Find out what users want

# ask user if they want to write-out files or not:
print_files = False
print_answer = input("\nWrite out files?(y/n) (Enter is no): ")
if print_answer in ["y", "Y", "yes", "YES", "Yes"]:
    print_files = True
print_files2 = True
print(f"Writing out files? {print_files}")

print_acks = False
print_ack_answer = input("\nWrite out acknowledgements?(y/n) (Enter is no): ")
if print_ack_answer in ["y", "Y", "yes", "YES", "Yes"]:
    print_acks = True
print(f"Writing out acknowledgements? {print_acks}")

division = False
division_answer = input("\nDo division for USA & Switzerland?(y/n) (Enter is no): ")
if division_answer in ["y", "Y", "yes", "YES", "Yes"]:
    division = True
    selected_country = ["USA", "Switzerland"]

# default is 222, but ask user what they want - or run all.
clus_to_run = ["EU1"]
reask = True

while reask:
    clus_answer = input(
        "\nWhat cluster to run? (Enter for S222) Type 'all' for all, type 'all mink' for all+mink: "
    )
    if len(clus_answer) != 0:
        if clus_answer in clusters.keys():
            print(f"Using {clus_answer}\n")
            clus_to_run = [clus_answer]
            reask = False
        elif "all" in clus_answer:
            clus_to_run = list(clusters.keys())
            if "mink" in clus_answer or "Mink" in clus_answer:
                clus_to_run.append("mink")
            reask = False
        elif clus_answer == "mink" or clus_answer == "Mink":
            clus_to_run = ["mink"]
        elif "," in clus_answer:
            answer_array = clus_answer.split(",")
            if all([x in clusters.keys() for x in answer_array]):
                print(f"Using {clus_answer}\n")
                clus_to_run = answer_array
                reask = False
            else:
                print(f"Not found. Options are: {clusters.keys()}")
        else:
            print(f"Not found. Options are: {clusters.keys()}")
    else:
        print("Using default of S222\n")
        reask = False
print("These clusters will be run: ", clus_to_run)

# ask user if they want to continue to do the full plotting - mostly we do these days 
do_country = False
if "all" in clus_answer:
    print_answer = input("\nContinue to country plotting? (y/n) (Enter is no): ")
    if print_answer in ["y", "Y", "yes", "YES", "Yes"]:
        do_country = True
else:
    print("Can't do country plot as aren't doing 'all' clusters")

if do_country == False:
    print(
        "You can alway run this step by calling `plot_country_data(clusters, proposed_coun_to_plot, print_files)`"
    )

do_divisions_country = False
if "all" in clus_answer:
    print_answer = input(
        "\nContinue to USA- & Swiss-specific country plotting? (y/n) (Enter is no): "
    )
    if print_answer in ["y", "Y", "yes", "YES", "Yes"]:
        do_divisions_country = True

exit_bad_dates = True
print_answer = input("\nExit the script when bad dates found during early check? (y/n) (Enter is yes): ")
if print_answer in ["n", "N", "no", "NO", "No"]:
    exit_bad_dates = False

start_time = time.time()

##################################
##################################
#### Read in the starting files

print("\nReading in files...\n")

t0 = time.time()

# Read metadata file
#input_meta = "data/downloaded_gisaid.tsv"  

#dtype = {
#    "strain": "str",
#    "virus": "category",
#    "gisaid_epi_isl": "str",
#    "genbank_accession": "str",
#    "sra_run_accession": "str",
#    "date": "category",
#    "region": "category",
#    "country": "category",
#    "division": "category",
#    "location": "category",
#    "region_exposure": "category",
#    "country_exposure": "category",
#    "division_exposure": "category",
#    "segment": "category",
#    "length": "int32",
#    "host": "category",
#    "age": "category",
#    "sex": "category",
#    "Nextstrain_clade": "category",
#    "pango_lineage": "category",
#    "GISAID_clade": "category",
#    "originating_lab": "category",
#    "submitting_lab": "category",
#    "submitting_lab": "category",
#    "authors": "category",
#    "url": "str",
#    "title": "str",
#    "paper_url": "str",
#    "date_submitted": "category",
#    "sampling_strategy": "category",
#    "missing_data": np.float32,
#    "divergence": np.float32,
#    "nonACGTN": np.float32,
#    "rare_mutations": np.float32,
#    "snp_clusters": np.float32,
#    "QC_missing_data": "category",
#    "QC_mixed_sites": "category",
#    "QC_rare_mutations": "category",
#    "QC_snp_clusters": "category",
#    "clock_deviation": "category"
#}
input_meta = "data/metadata.tsv"
dtype={'location': str, 'sampling_strategy': str, 'clock_deviation': str, 'age': str, 'QC_frame_shifts': str, 'frame_shifts': str}
cols = ['strain', 'date', 'division', 'host', 'substitutions', 'deletions', 'Nextstrain_clade', 'country', 'gisaid_epi_isl']
meta = pd.read_csv(input_meta, sep="\t", dtype=dtype, index_col=False, usecols=cols) #dtype={'location': str, 'sampling_strategy': str, 'clock_deviation': str}, index_col=False)
meta = meta.fillna("")

# Clean up metadata

print("\nCleaning up the metadata...\n")

# If bad seq there  - exclude!
maybe_bad = meta.loc[meta['strain'].isin(bad_seqs.keys())]
really_bad = [s.date==bad_seqs[s.strain] for ri,s in maybe_bad.iterrows()]
bad_indices =  maybe_bad.loc[really_bad].index
meta.drop(bad_indices, inplace=True)

bads = meta[meta['strain'].isin(bad_seqs.keys())]

# do some modifications to metadata once, here - to exclude bad dates
meta = meta[meta["date"].apply(lambda x: len(x) == 10)]
meta = meta[meta["date"].apply(lambda x: "XX" not in x)]

meta["date_formatted"] = meta["date"].apply(
    lambda x: datetime.datetime.strptime(x, "%Y-%m-%d")
)

# warn of any in the future
future_meta = meta[meta["date_formatted"].apply(lambda x: x > datetime.date.today())]
if not future_meta.empty:
    print("WARNING! Data from the future!")
    print(future_meta)
# get rid of any with dates in the future.....
meta = meta[meta["date_formatted"].apply(lambda x: x <= datetime.date.today())]

# Replace Swiss divisions with swiss-region, but store original division
meta['orig_division'] = meta['division']
meta['division'] = meta['division'].replace(swiss_regions)
# meta[meta["country"].apply(lambda x: x == "Switzerland")]  #can check

# Filter for only Host = Human
meta = meta[meta["host"] == "Human"]

# Filter Metadata to only have those we have mutations for! Allows 'out of sync' files. --> Should we check for the subtitutions and deletions columns to be not empty?
# meta = meta[meta["strain"].isin(muts["Unnamed: 0"])]

t1 = time.time()
print(f"Reading & cleaning meta run took {round((t1-t0)/60,1)} min to run")

print("\nMetadata is ready to go...\n")

# Figure out what clades are really recognised
official_clades = list(meta["Nextstrain_clade"].unique())

# Search for early dates
t0 = time.time()

# Adjust to display names for easier check in meta dataframe (can directly compare to "Nextstrain_clade")
clus_dates = {clusters[clus]["display_name"] : datetime.datetime.strptime(cluster_first_dates[clus]["first_date"], "%Y-%m-%d") for clus in clus_to_run if clus in cluster_first_dates}

# Search for sequences with too early dates - only for Nextstrain_clades
before_date = meta[[date < clus_dates[clade] if clade in clus_dates else False for date,clade in zip(meta["date_formatted"], meta["Nextstrain_clade"])]]

# Sort by clus
alert_first_date_quick = {clus : before_date[before_date["Nextstrain_clade"] == clusters[clus]["display_name"]] for clus in clus_to_run if clusters[clus]["display_name"] in before_date["Nextstrain_clade"].values}

if alert_first_date_quick:
    print("\nDate alerts (fast check):")
    print([f"{x}: {len(alert_first_date_quick[x])}" for x in alert_first_date_quick.keys()])
    print("To view, use 'print_all_date_alerts_quick()' or 'print_date_alerts_quick(<clus>)'\n")
    if exit_bad_dates:
        sys.exit("Bad dates found. Exit program...")
else:
    print("\nNo bad dates found.\n")

t1 = time.time()
print(f"Checking for early dates took {round((t1-t0)/60,1)} min to run")

##################################
##################################
#### Prepare output files

json_output = {}

# if running all clusters, clear file so can write again.
if print_files and "all" in clus_answer:
    #empty file to write clean
    with open(overall_tables_file, "w") as fh:
        fh.write("\n")

    curPath = cluster_path + "clusters/current/"
    for f in os.listdir(curPath):
        if os.path.isfile(curPath + f):
            os.remove(curPath + f)


######################################################################################################
##################################
#### Actually start running things

##################################
##################################
#### Ensure we have all the variable we need for each run

for clus in clus_to_run:
    print(f"\nGathering cluster {clus}\n")
    clus_data = clusters[clus]

    if clus == "mink":
        clus_build_name = "mink"
        clus_data["mink_meta"] = meta[meta["host"].apply(lambda x: x == "Mink")]
        clus_data["wanted_seqs"] = list(clus_data["mink_meta"]["strain"])

        clus_data["clusterlist_output"] = cluster_path + f"/clusters/cluster_mink.txt"
        clus_data["out_meta_file"] = (
            cluster_path + f"/cluster_info/cluster_mink_meta.tsv"
        )

    else:
        clus_build_name = clusters[clus]["build_name"]
        snps = clusters[clus]["snps"]
        if "snps2" not in clusters[clus]:
            clus_data["snps2"] = []
        if "gaps" not in clusters[clus]:
            clus_data["gaps"] = []
        if "exclude_snps" not in clusters[clus]:
            clus_data["exclude_snps"] = []

        clus_data["wanted_seqs"] = []

        clus_data["clusterlist_output"] = (
            cluster_path + f'/clusters/cluster_{clusters[clus]["build_name"]}.txt'
        )
        clus_data["out_meta_file"] = (
            cluster_path
            + f'/cluster_info/cluster_{clusters[clus]["build_name"]}_meta.tsv'
        )


##################################
##################################
#### For all but mink, go through and extract wanted sequences

t0 = time.time()

print("\nLooking for the wanted sequences in the file...\n")

#muts["snp_pos"] = muts.nucleotide.fillna('').apply(lambda x: [int(y[1:-1]) for y in x.split(',') if y and y[-1] in 'ACGT'])
#muts["gap_pos"] = muts.nucleotide.fillna('').apply(lambda x: [int(y[1:-1]) for y in x.split(',') if y and y[-1] in '-'])
muts_snp_pos = meta.substitutions.fillna('').apply(lambda x: [int(y[1:-1]) for y in x.split(',') if y])

# expand metadata deletions formatting
muts_del_pos = meta.deletions.fillna('').apply(lambda x: [z for y in x.split(',') if y for z in range(int(y.split("-")[0]), int(y.split("-")[-1]) + 1)])


# If an official Nextstrain clade, then use Nextclade designation to find them.
# If not an official Nextstrain clade, use our SNP method

n_done = 1
for clus in [x for x in clus_to_run if x != "mink"]:
    clus_data = clusters[clus]
    snps = clus_data["snps"]
    snps2 = clus_data["snps2"]
    gaps = clus_data["gaps"]
    display_name = clus_data['display_name']
    exclude_snps = clus_data["exclude_snps"]
    wanted_seqs = clus_data["wanted_seqs"]

    # Use Nextclade
    if "other_nextstrain_names" in clus_data:
        next_names = clus_data["other_nextstrain_names"]
        for next_na in next_names:
            next_assign = meta[meta["Nextstrain_clade"].apply(lambda x: x == next_na)]
            wanted_seqs.extend(list(next_assign.strain))

    elif display_name in official_clades:
        next_assign = meta[meta["Nextstrain_clade"].apply(lambda x: x == display_name)]
        wanted_seqs.extend(list(next_assign.strain))

    else:
    #Use SNPS

        # look for occurance of snp(s) *without* some other snp(s) (to exclude a certain group)
        if snps:
            founds = meta.loc[muts_snp_pos.apply(lambda x: all((p in x) for p in snps) & all((p not in x) for p in exclude_snps)),'strain']
            wanted_seqs.extend(founds)

        # look for additional occurances which have snps2
        # (to look for 2 muts that affect same AA, for example)
        if snps2:
            founds = meta.loc[muts_snp_pos.apply(lambda x: all((p in x) for p in snps2) & all((p not in x) for p in exclude_snps)),'strain']
            wanted_seqs.extend(founds)

        #look for sequences by gaps
        if gaps:
            founds = meta.loc[muts_del_pos.apply(lambda x: all((p in x) for p in gaps)),'strain']
            wanted_seqs.extend(founds)

    #dedup
    wanted_seqs = list(set(wanted_seqs))
    t1a = time.time()
    print(f"Completed {n_done} out of {len(clus_to_run)}: {round((t1a-t0)/60,1)} min")
    n_done += 1


t1 = time.time()
print(f"Finding sequences took {round((t1-t0)/60,1)} min to run")

nextstrain_name_to_clus = {clusters[clus]["nextstrain_name"]: clus for clus in clus_to_run if "nextstrain_name" in clusters[clus]}

##################################
##################################
#### Gather metadata
t0 = time.time()
n_done = 1
for clus in clus_to_run:
    print(f"\nGathering metadata for cluster {clus}, cluster {n_done} out of {len(clus_to_run)}\n")

    clus_data = clusters[clus]
    wanted_seqs = clus_data["wanted_seqs"]
    clus_build_name = clus_data["build_name"]
    display_cluster = clus_data["display_name"]
    clusterlist_output = clus_data["clusterlist_output"]
    out_meta_file = clus_data["out_meta_file"]

    json_output[clus_build_name] = {}

    # get metadata for these sequences
    cluster_meta = meta[meta["strain"].isin(wanted_seqs)].copy()
    clus_data["cluster_meta"] = cluster_meta

    # if wanted seqs are part of a Nextclade designated variant, remove from that count & use this one.
    # ONLY IF PLOTTING and if this run ISN'T an official run
    if clus_data["graphing"] and "nextstrain_name" not in clus_data:
        print(f"Removing {clus} samples from other Nextstrain builds")
        clades_double = cluster_meta["Nextstrain_clade"].unique()
        clades_to_remove = [x for x in clades_double if x in nextstrain_name_to_clus]

        for cla in clades_to_remove:
            print(f"Removing {clus} samples from {cla}")
            other_meta = clusters[nextstrain_name_to_clus[cla]]["cluster_meta"]
            other_wanted = clusters[nextstrain_name_to_clus[cla]]["wanted_seqs"]
            #get the ones to remove from the other cluster 'record'
            to_remove = cluster_meta[cluster_meta["Nextstrain_clade"] == cla].strain
            #remove from meta
            bad_seqs = other_meta.loc[other_meta['strain'].isin(to_remove)]
            bad_indices = bad_seqs.index
            other_meta.drop(bad_indices, inplace=True)
            #remove from wanted list
            new_wanted = list(set(other_wanted) - set(to_remove))
            #copy back
            clusters[nextstrain_name_to_clus[cla]]["wanted_seqs"] = new_wanted


    # re-set wanted_seqs
    wanted_seqs = list(cluster_meta["strain"])

    print("Sequences found: ")
    print(len(wanted_seqs))  # how many are there?
    print("\n")

    #Do we want to write out cluster for Nextstrain?
    if print_files:
        nextstrain_run = clusters[clus]['nextstrain_build']
    else:
        nextstrain_run = False

    # Write out a file of the names of those 'in the cluster' - this is used by ncov_cluster
    # to make a ncov run where the 'focal' set is this cluster.
    if print_files and nextstrain_run:
        with open(clusterlist_output, "w") as f:
            for item in wanted_seqs:
                f.write("%s\n" % item)

        # Copy file with date, so we can compare to prev dates if we want...
        if clus in clusters:
            build_nam = clusters[clus]["build_name"]
        else:
            build_nam = "mink"
        copypath = clusterlist_output.replace(
            f"{build_nam}",
            "{}-{}".format(build_nam, datetime.date.today().strftime("%Y-%m-%d")),
        )
        copyfile(clusterlist_output, copypath)
        copypath2 = clusterlist_output.replace(
            "clusters/cluster_", "clusters/current/cluster_"
        )
        copyfile(clusterlist_output, copypath2)

        # Just so we have the data, write out the metadata for these sequences
        cluster_meta.to_csv(out_meta_file, sep="\t", index=False)

    # If specified wanted dated Q677, do this
    #if print_files and dated_limit and "Q677" in display_cluster:
    # if want a dated limit, specify cluster and limit at top
    if print_files and dated_limit and dated_cluster in display_cluster:
    ####### if dated_limit and "Q677" in clus_build_name: # (old)
        build_nam = clusters[clus]["build_name"]
        dated_clus_met = cluster_meta[cluster_meta["date_formatted"].apply(lambda x: x < datetime.datetime.strptime(dated_limit, "%Y-%m-%d"))]
        dated_want_seqs = list(dated_clus_met["strain"])

        datedpath = clusterlist_output.replace(
            f"{build_nam}",
            "{}-{}".format(build_nam, dated_limit),
        )
        curr_datedpath = datedpath.replace(
            "clusters/cluster_", "clusters/current/cluster_"
        )
        with open(curr_datedpath, "w") as f:
            for item in dated_want_seqs:
                f.write("%s\n" % item)


    # What countries do sequences in the cluster come from?
    #observed_countries = [x for x in cluster_meta["country"].unique() if x]
    observed_countries = list(cluster_meta.country.unique())
    clus_data["observed_countries"] = observed_countries
    # Use below to print observed countries, if interested
    #print(f"The cluster is found in: {observed_countries}\n")

    # Let's get some summary stats on number of sequences, first, and last, for each country.
    country_info, country_dates = get_summary(cluster_meta, observed_countries)
    print(country_info)
    print("\n")
    clus_data["country_info"] = country_info
    clus_data["country_dates"] = country_dates

    # make into a dataframe for sorting
    country_info_df = pd.DataFrame(data=country_info)
    clus_data["country_info_df"] = country_info_df

    print("\nOrdered list by first_seq date:")
    print(country_info_df.sort_values(by="first_seq"))
    country_info_df = country_info_df.sort_values(by="first_seq")
    print("\n")

    ii = 0
    firstseqdate = country_info_df["first_seq"][ii]
    while datetime.datetime.strptime(firstseqdate, "%Y-%m-%d") < datetime.datetime.strptime("2019-12-01", "%Y-%m-%d"):
        ii += 1
        firstseqdate = country_info_df["first_seq"][ii]

    if clus in cluster_first_dates:
        # datetime.datetime.strptime(cluster_first_dates[clus]["first_date"],"%Y-%m-%d")
        alert = None
        clus_start = datetime.datetime.strptime(cluster_first_dates[clus]["first_date"], "%Y-%m-%d")
        if datetime.datetime.strptime(firstseqdate, "%Y-%m-%d") < clus_start:
            alert = f"ALERT: sequence date of {firstseqdate}"

            before_date = cluster_meta[cluster_meta["date_formatted"].apply(lambda x: x < clus_start)]

        if alert:
            alert_first_date[clus] = before_date

    #Stop making this build - 23 Feb 2022
    # Make a version of Delta which does not have as much UK/India sequences for increased viewability
    #if clus == "21AS478":
    #if clus_build_name == "21A.Delta":
    #    nouk_delta_meta = cluster_meta[
    #        cluster_meta["country"].apply(lambda x: x != "United Kingdom" and x != "India")
    #    ]
    #    # re-set wanted_seqs
    #    extraDelta_wanted_seqs = list(nouk_delta_meta["strain"])
#
    #    noUK_clusterlist_output = (
    #        cluster_path + f'/clusters/cluster_{clusters[clus]["build_name"]}-noUKIndia.txt'
    #    )
    #    noUK_out_meta_file = (
    #        cluster_path
    #        + f'/cluster_info/cluster_{clusters[clus]["build_name"]}-noUKIndia_meta.tsv'
    #    )
#
    #    if print_files:
    #        with open(noUK_clusterlist_output, "w") as f:
    #            for item in extraDelta_wanted_seqs:
    #                f.write("%s\n" % item)
    #        build_nam = clusters[clus]["build_name"]
    #        copypath = noUK_clusterlist_output.replace(
    #            f"{build_nam}-noUKIndia",
    #            "{}-noUKIndia-{}".format(
    #                build_nam, datetime.date.today().strftime("%Y-%m-%d")
    #            ),
    #        )
    #        copyfile(noUK_clusterlist_output, copypath)
    #        nouk_delta_meta.to_csv(noUK_out_meta_file, sep="\t", index=False)
    #        copypath2 = noUK_clusterlist_output.replace(
    #            "clusters/cluster_", "clusters/current/cluster_"
    #        )
    #        copyfile(noUK_clusterlist_output, copypath2)

    # Make a version of Delta for Europe
    ### DISABLED because there are now 3 delta categories -no way to pull Europe seqs from each.
    #if clus == "21AS478":
#    if clus_build_name == "21A.Delta":
#        eu_delta_meta = cluster_meta[
#            cluster_meta["region"].apply(lambda x: x == "Europe")
#        ]
#        # re-set wanted_seqs
#        extraDelta_wanted_seqs = list(eu_delta_meta["strain"])
#
#        eu_clusterlist_output = (
#            cluster_path + f'/clusters/cluster_{clusters[clus]["build_name"]}-europe.txt'
#        )
#        eu_out_meta_file = (
#            cluster_path
#            + f'/cluster_info/cluster_{clusters[clus]["build_name"]}-europe_meta.tsv'
#        )
#
#        if print_files:
#            with open(eu_clusterlist_output, "w") as f:
#                for item in extraDelta_wanted_seqs:
#                    f.write("%s\n" % item)
#            build_nam = clusters[clus]["build_name"]
#            copypath = eu_clusterlist_output.replace(
#                f"{build_nam}-europe",
#                "{}-europe-{}".format(
#                    build_nam, datetime.date.today().strftime("%Y-%m-%d")
#                ),
#            )
#            copyfile(eu_clusterlist_output, copypath)
#            eu_delta_meta.to_csv(eu_out_meta_file, sep="\t", index=False)
#            copypath2 = eu_clusterlist_output.replace(
#                "clusters/cluster_", "clusters/current/cluster_"
#            )
#            copyfile(eu_clusterlist_output, copypath2)

    #Stop making these builds, 23 Feb 2022 - currently only for A,B,G & these are not worth updating as variants ess. gone
    # Make a version of  Alpha-Delta which only have Swiss sequences for increased focus
    #if clus in ["501YV1", "501YV2", "501YV3", "21AS478"]:
    ## DISABLED for DELTA because now 3 categories of Delta - no way to pull seqs from all of them...
#    if clus_build_name in ["20I.Alpha.V1", "20H.Beta.V2", "20J.Gamma.V3" ]: #, "21A.Delta"]:
#        swiss_voc_meta = cluster_meta[
#            cluster_meta["country"].apply(lambda x: x == "Switzerland")
#        ]
#        # re-set wanted_seqs
#        extravoc_wanted_seqs = list(swiss_voc_meta["strain"])
#
#        swissvoc_clusterlist_output = (
#            cluster_path
#            + f'/clusters/cluster_{clusters[clus]["build_name"]}-swiss.txt'
#        )
#        swissvoc_out_meta_file = (
#            cluster_path
#            + f'/cluster_info/cluster_{clusters[clus]["build_name"]}-swiss_meta.tsv'
#        )
#
#        if print_files:
#            with open(swissvoc_clusterlist_output, "w") as f:
#                for item in extravoc_wanted_seqs:
#                    f.write("%s\n" % item)
#            build_nam = clusters[clus]["build_name"]
#            copypath = swissvoc_clusterlist_output.replace(
#                f"{build_nam}-swiss",
#                "{}-swiss-{}".format(
#                    build_nam, datetime.date.today().strftime("%Y-%m-%d")
#                ),
#            )
#            copyfile(swissvoc_clusterlist_output, copypath)
#            swiss_voc_meta.to_csv(swissvoc_out_meta_file, sep="\t", index=False)
#            copypath2 = swissvoc_clusterlist_output.replace(
#                "clusters/cluster_", "clusters/current/cluster_"
#            )
#            copyfile(swissvoc_clusterlist_output, copypath2)

    #######
    # print out the table
    table_file = f"{tables_path}{clus_build_name}_table.tsv"
    ordered_country = country_info_df.sort_values(by="first_seq")
    clus_data["country_info_ordered"] = ordered_country

    if print_files:
        ordered_country.to_csv(table_file, sep="\t")
        # only write if doing all clusters
        if "all" in clus_answer:
            with open(overall_tables_file, "a") as fh:
                fh.write(f"\n\n## {display_cluster}\n")
            ordered_country.to_csv(overall_tables_file, sep="\t", mode="a")
        mrk_tbl = ordered_country.to_markdown()

        nextstrain_url = clusters[clus]["nextstrain_url"]
        n_done += 1

    if print_acks:
        # remove all but EPI_ISL, on request from GISAID
        # acknowledgement_table = cluster_meta.loc[:,['strain', 'gisaid_epi_isl', 'originating_lab', 'submitting_lab', 'authors']]
        acknowledgement_table = cluster_meta.loc[:, ["gisaid_epi_isl"]]
        # do not put in acknowledgement folder, on request from GISAID
        # acknowledgement_table.to_csv(f'{acknowledgement_folder}{clus}_acknowledgement_table.tsv', sep="\t")
        if clus_build_name != "DanishCluster":
            acknowledgement_by_variant["acknowledgements"][
                clus_build_name
            ] = cluster_meta.loc[:, ["gisaid_epi_isl"]]["gisaid_epi_isl"].tolist()

        # only do this for 'all' runs as otherwise the main file won't be updated.
        if clus_build_name != "DanishCluster" and "all" in clus_answer:
            ack_out_folder = acknowledgement_folder_new + f"{clus_build_name}/"
            if not os.path.exists(ack_out_folder):
                os.mkdir(ack_out_folder)
            ack_list = acknowledgement_by_variant["acknowledgements"][clus_build_name]
            chunk_size = 1000
            chunks = [
                ack_list[i : i + chunk_size]
                for i in range(0, len(ack_list), chunk_size)
            ]

            # get number & file names
            ack_file_names = ["{0:03}".format(i) for i in range(len(chunks))]
            acknowledgement_keys["acknowledgements"][clus_build_name] = {}
            acknowledgement_keys["acknowledgements"][clus_build_name]["numChunks"] = len(
                chunks
            )

            for ch, fn in zip(chunks, ack_file_names):
                with open(ack_out_folder + fn + ".json", "w") as fh:
                    json.dump(ch, fh, indent=2, sort_keys=True)


# only print if doing 'all' or it'll overwrite a multi-variant file with just one var.
# if print_acks and "all" in clus_answer:
#    with open(acknowledgement_folder_new+'acknowledgements_all.json', 'w') as fh:
#        json.dump(acknowledgement_by_variant, fh, indent=2, sort_keys=True)

if print_acks and "all" in clus_answer:
    with open(acknowledgement_folder_new + "acknowledgements_keys.json", "w") as fh:
        json.dump(acknowledgement_keys, fh, indent=2, sort_keys=True)

t1 = time.time()
print(f"Gathering metadata for all clusters took {round((t1-t0)/60,1)} min to run\n")

####################################################################
##################################
# Get ALL sequence counts for ALL countries ONCE

print("\nGathering up total sequences (only 2 week period)")

# Don't use isocalendar, just compare to reference Monday
ref_monday = datetime.datetime.strptime("2020-04-27", '%Y-%m-%d').toordinal()
def to2week_ordinal(x):
    n = x.toordinal()
    monday = datetime.date.fromordinal(n - ((n - ref_monday) % 14))
    return (monday.isocalendar()[0], monday.isocalendar()[1]) #TODO: Currently returned as tuple of year & week -> Can we switch to returning datetime? Needs adjustment at several places in the script

t0 = time.time()

all_sequence_counts = {}
all_observed_countries = list(meta.country.unique())
for coun in all_observed_countries:
    temp_meta = meta[meta["country"].apply(lambda x: x == coun)].copy()

    # TWO WEEKS
    temp_meta["calendar_2week"] = temp_meta["date_formatted"].apply(to2week_ordinal)
    temp_meta = temp_meta[temp_meta["calendar_2week"] >= min_data_week]
    if temp_meta.empty:
        continue

    week2_counts = temp_meta["calendar_2week"].value_counts().sort_index()
    counts_by_2week = week2_counts.to_dict()
    all_sequence_counts[coun] = {}
    all_sequence_counts[coun]['2week'] = counts_by_2week

##################################
##################################
# if division, get all sequence counts per division
division_all_sequence_counts = {}
if division:
    for sel_coun in selected_country:
        division_all_sequence_counts[sel_coun] = {}
        # use list comprehension to get rid of empty divisions!!!
        observed_divisions = [x for x in meta[meta["country"].apply(lambda x: x == sel_coun)].division.unique() if x]
    
        for div in observed_divisions:
            temp_meta = meta[meta["division"].apply(lambda x: x == div)].copy()

            # TWO WEEKS
            temp_meta["calendar_2week"] = temp_meta["date_formatted"].apply(to2week_ordinal)
            temp_meta = temp_meta[temp_meta["calendar_2week"] >= min_data_week]
            if temp_meta.empty:
                continue

            week2_counts = temp_meta["calendar_2week"].value_counts().sort_index()
            counts_by_2week = week2_counts.to_dict()
            division_all_sequence_counts[sel_coun][div] = {}
            division_all_sequence_counts[sel_coun][div]['2week'] = counts_by_2week

t1 = time.time()
print(f"Gathering up sequences took {round((t1-t0)/60,1)} min to run (partway through binning)\n")

######################################################################################################
##################################
#### PREPARING FOR OF PLOTTING - putting clusters into date bins

for clus in clus_to_run:
    print(f"\nPutting {clus} data into bins by date\n")

    clus_data = clusters[clus]
    wanted_seqs = clus_data["wanted_seqs"]
    clus_build_name = clus_data["build_name"]
    cluster_meta = clus_data["cluster_meta"]
    observed_countries = clus_data["observed_countries"]
    country_dates = clus_data["country_dates"]
    country_info_df = clus_data["country_info_ordered"]

    if division:
        for sel_coun in selected_country:
            clus_data[sel_coun] = {}
            clus_data[sel_coun]["cluster_data_div"] = []
            clus_data[sel_coun]["total_data_div"] = []

            # divide out data for divisions
            cluster_meta_div = cluster_meta[
                cluster_meta["country"].apply(lambda x: x == sel_coun)
            ]
            observed_divisions = [x for x in cluster_meta_div["division"].unique() if x]

            division_info, division_dates = get_summary(
                cluster_meta_div, observed_divisions, division=True
            )

            clus_data[sel_coun]["observed_divisions"] = observed_divisions
            clus_data[sel_coun]["division_info"] = division_info
            clus_data[sel_coun]["division_dates"] = division_dates

    # We want to look at % of samples from a country that are in this cluster
    # To avoid the up-and-down of dates, bin samples into weeks
    countries_to_plot = country_list
    acknowledgement_table = []
    clus_data["acknowledgements"] = acknowledgement_table

    # COUNTRY LEVEL
    # Get counts per week for sequences in the cluster
    clus_2week_counts = {}

    # converts to tuple (year, ISO calendar week - every 2 weeks)
    cluster_meta["yr_2wk"] = cluster_meta.date_formatted.apply(to2week_ordinal)

    for coun in observed_countries:
        temp = cluster_meta[cluster_meta["country"].apply(lambda x: x == coun)]
        clus_2week_counts[coun] = temp.value_counts(subset="yr_2wk").sort_index().to_dict()

    # Get counts per week for sequences regardless of whether in the cluster or not - from week 20 only.
    total_2week_counts = {}
    for coun in observed_countries:
        total_2week_counts[coun] = all_sequence_counts[coun]['2week']

    # COUNTRY LEVEL

    # Convert into dataframe -- 2 weeks
    cluster_data_2wk = pd.DataFrame(data=clus_2week_counts)
    total_data_2wk = pd.DataFrame(data=total_2week_counts)
    # sort
    total_data_2wk = total_data_2wk.sort_index()
    cluster_data_2wk = cluster_data_2wk.sort_index()
    # store
    clus_data["cluster_data_2wk"] = cluster_data_2wk
    clus_data["total_data_2wk"] = total_data_2wk

    ###########################################
    # DIVISION LEVEL
    # Get counts per week for sequences in the cluster
    if division:
        for sel_coun in selected_country:
            clus_2week_counts_div = {}
            observed_divisions = clus_data[sel_coun]["observed_divisions"] 
            for div in observed_divisions:
                temp = cluster_meta[cluster_meta["division"].apply(lambda x: x == div)]
                clus_2week_counts_div[div] = temp.value_counts(subset="yr_2wk").sort_index().to_dict()

            # Get counts per week for sequences regardless of whether in the cluster or not - from week 20 only.
            total_2week_counts_div = {}
            for div in observed_divisions:
                total_2week_counts_div[div] = division_all_sequence_counts[sel_coun][div]['2week']

            # DIVISION LEVEL

            # Convert into dataframe -- 2 weeks
            cluster_data_2wk_div = pd.DataFrame(data=clus_2week_counts_div)
            total_data_2wk_div = pd.DataFrame(data=total_2week_counts_div)
            # sort
            total_data_2wk_div = total_data_2wk_div.sort_index()
            cluster_data_2wk_div = cluster_data_2wk_div.sort_index()
            # store
            clus_data[sel_coun]["cluster_data_2wk_div"] = cluster_data_2wk_div
            clus_data[sel_coun]["total_data_2wk_div"] = total_data_2wk_div

t1 = time.time()
print(f"Date-binning data took {round((t1-t0)/60,1)} min to run\n\n")

######################################################################################################
##################################
#### FIGURE OUT WHAT TO PLOT

t0 = time.time()

cutoff_num_seqs = 1000

# This prints countries with more than cutoff_num_seqs PER CLUSTER - messy output.
clusters_tww = []
for clus in clus_to_run:
    c_i = clusters[clus]["country_info"]
    c_i[c_i["num_seqs"] > cutoff_num_seqs]
    #print(f"Countries with >{cutoff_num_seqs} seqs in cluster {clus}:")
    #print("\t", ", ".join(c_i[c_i["num_seqs"] > 10].index))
    if len(c_i[c_i["num_seqs"] > 10]) > 0 and clus_build_name != "DanishCluster":
        clusters_tww.append(clus)
    print("")


# fix cluster order in a list so it's reliable
clus_keys = [x for x in clus_to_run if x in clusters_tww]

my_df = [clusters[x]["country_info"] for x in clus_keys]
all_num_seqs = pd.concat([x.loc[:, "num_seqs"] for x in my_df], axis=1)
all_num_seqs.columns = clus_keys

has10 = []
has10_countries = []
for index, row in all_num_seqs.iterrows():
    if any(row > cutoff_num_seqs) and index not in uk_countries:
        has10.append("*")
        has10_countries.append(index)
    else:
        has10.append("")

all_num_seqs["has_20"] = has10

print(
    f"Countries who have more than {cutoff_num_seqs} in any cluster:",
    has10_countries,
    "\n",
    f"There are {len(has10_countries)}",
    "\n",
)
# This prints table of all countries & all clusters and marks those with >cutoff_num_seqs
# print(all_num_seqs)

countries_to_plot_final = all_num_seqs[all_num_seqs.has_20 == "*"].index

t1 = time.time()
print(f"Figuring out what fits plotting criteria (n of seqs) took {round((t1-t0)/60,1)} min to run\n\n")

######################################################################################################
##################################
#### BEGINNING OF PLOTTING

t0 = time.time()

countries_plotted = {}

ndone = 1
for clus in clus_to_run:

    print(f"\nPlotting & writing out cluster {clus}: number {ndone} of {len(clus_to_run)}")

    clus_data = clusters[clus]
    wanted_seqs = clus_data["wanted_seqs"]
    clus_build_name = clus_data["build_name"]
    cluster_meta = clus_data["cluster_meta"]
    observed_countries = clus_data["observed_countries"]
    country_dates = clus_data["country_dates"]
    country_info_df = clus_data["country_info_ordered"]
    cluster_data = clus_data["cluster_data_2wk"]
    total_data = clus_data["total_data_2wk"]

    width = 1
    smoothing = np.exp(-np.arange(-10, 10) ** 2 / 2 / width ** 2)
    smoothing /= smoothing.sum()

    # Only plot countries with >= X seqs
    min_to_plot = cutoff_num_seqs

    countries_to_plot_min = country_info_df[
        country_info_df.num_seqs > min_to_plot
    ].index

    #only plot those over >=X seqs - or if already going to plot elsewhere!
    countries_to_plot_min = [x for x in country_info_df.index if (x in countries_to_plot_final or country_info_df.num_seqs[x] > min_to_plot)]

    countries_to_plot = [
        x
        for x in country_info_df[country_info_df.num_seqs > min_to_plot].index
        if x in countries_to_plot_final
    ]

    for coun in [x for x in countries_to_plot_min]:
        (
            week_as_date,
            cluster_count,
            total_count,
            unsmoothed_cluster_count,
            unsmoothed_total_count,
        ) = non_zero_counts(cluster_data, total_data, coun, smoothing=smoothing)
        # remove last data point if that point as less than frac sequences compared to the previous count
        week_as_date, cluster_count, total_count = trim_last_data_point(
            week_as_date, cluster_count, total_count, frac=0.1, keep_count=10
        )
        if len(cluster_count) < len(
            unsmoothed_cluster_count
        ):  # if the trim_last_data_point came true, match trimming
            unsmoothed_cluster_count = unsmoothed_cluster_count[:-1]
            unsmoothed_total_count = unsmoothed_total_count[:-1]

        json_output[clus_build_name][coun] = {}
        json_output[clus_build_name][coun]["week"] = [
            datetime.datetime.strftime(x, "%Y-%m-%d") for x in week_as_date
        ]
        json_output[clus_build_name][coun]["total_sequences"] = [
            int(x) for x in total_count
        ]
        json_output[clus_build_name][coun]["cluster_sequences"] = [
            int(x) for x in cluster_count
        ]
        json_output[clus_build_name][coun]["unsmoothed_cluster_sequences"] = [
            int(x) for x in unsmoothed_cluster_count
        ]
        json_output[clus_build_name][coun]["unsmoothed_total_sequences"] = [
            int(x) for x in unsmoothed_total_count
        ]

        # This used to only plot a subset (those in 'countries to plot' below.)
        # However for a long time (as of Jul 21) all have been selected,
        # And I think this is most intuitive - so setting all to 'True'
        countries_plotted[coun] = "True" #"False"

        if print_files:
            with open(tables_path + f"{clus_build_name}_data.json", "w") as fh:
                json.dump(json_output[clus_build_name], fh)
    ndone += 1

if "all" in clus_answer:
    for coun in countries_plotted.keys():
        if coun not in country_styles_all:
            print(
                f"WARNING!: {coun} has no color! Please add it to country_list_2 in colors_and_countries.py and re-run make web-data"
            )

if "all" in clus_answer:
    for coun in country_styles_all:
        if coun not in countries_plotted.keys():
            print(f"Not plotted anymore: {coun}")

## Write out plotting information - only if all clusters have run
if print_files and "all" in clus_answer:
    with open(tables_path + f"perVariant_countries_toPlot.json", "w") as fh:
        json.dump(countries_plotted, fh)

print("Date alerts:")
print([f"{x}: {len(alert_first_date[x])}" for x in alert_first_date.keys()])
print("To view, use 'alert_first_date[clus][['strain','date']]'")

t1 = time.time()
print(f"Plotting & writing out cluster took {round((t1-t0)/60,1)} min to run\n\n")

end_time = time.time()

print("\n\n############################")
print(f"The whole analysis so far took {round((end_time-start_time)/60,1)} min to run")
print("############################\n\n")

#####################################################################
#####################################################################
#####################################################################
#####################################################################
# Start of the country data/plotting



        

def get_ordered_clusters_to_plot(clusters, division=False, selected_country=None):
    # fix cluster order in a list so it's reliable
    clus_keys = [x for x in clusters.keys()]  # if x in clusters_tww]

    # DO NOT PLOT 69 AS IT OVERLAPS WITH 439 AND 501!!!!
    # Do not plot 484 as it overlaps with 501Y.V2, possibly others
    # Do not plot DanishCluster as also overlaps
    # clus_keys = [x for x in clus_keys if x not in ["S69","S484", "DanishCluster"]]
    if division:
        clus_keys = [
            x
            for x in clus_keys
            if clusters[x]["type"] == "variant"
            or ("usa_graph" in clusters[x] and clusters[x]["usa_graph"] is True)
        ]
        cluster_data_key = "cluster_data_2wk_div"
        min_to_plot = 20
    else:
        clus_keys = [x for x in clus_keys if clusters[x]["graphing"] is True]
        cluster_data_key = "cluster_data_2wk"
        min_to_plot = 70

    countries_all = defaultdict(dict)
    for clus in clus_keys:
        if division:
            clus_dat = clusters[clus][selected_country][cluster_data_key]
        else:
            clus_dat = clusters[clus][cluster_data_key]
        for coun in clus_dat.columns:
            countries_all[coun][clus] = clus_dat[coun]

    # how to decide what to plot?
    proposed_coun_to_plot = []
    for clus in clus_keys:
        if division:
            country_inf = clusters[clus][selected_country]["division_info"]
        else:
            country_inf = clusters[clus]["country_info_df"]
        proposed_coun_to_plot.extend(
            country_inf[country_inf.num_seqs > min_to_plot].index
        )
    proposed_coun_to_plot = set(proposed_coun_to_plot)
    print(f"At min plot {min_to_plot}, there are {len(proposed_coun_to_plot)} entries")

    total_coun_counts = {}
    # decide order
    for clus in clus_keys:
        if division:
            country_inf = clusters[clus][selected_country]["division_info"]
        else:
            country_inf = clusters[clus]["country_info_df"]
        for coun in proposed_coun_to_plot:
            if coun not in total_coun_counts:
                total_coun_counts[coun] = 0
            if coun in country_inf.index:
                total_coun_counts[coun] += country_inf.num_seqs[coun]

    sorted_country_tups = sorted(
        total_coun_counts.items(), key=lambda x: x[1], reverse=True
    )
    proposed_coun_to_plot = [x[0] for x in sorted_country_tups]

    return proposed_coun_to_plot, clus_keys


def plot_country_data(
    clusters,
    proposed_coun_to_plot,
    print_files,
    clus_keys,
    file_prefix,
    division=False,
    selected_country=None,
):
    country_week = {clus: {} for clus in clusters}


    min_week = datetime.datetime(2020, 12, 31)
    max_week = datetime.datetime(2020, 1, 1)
    week_as_dates = {}
    json_output = {}
    json_output["countries"] = {}

    for coun in proposed_coun_to_plot:
        i = 0
        first_clus_count = []

        country_data = {"week": {}, "total_sequences": {}}

        for clus in clus_keys:
            if division:
                cluster_data = clusters[clus][selected_country]["cluster_data_2wk_div"]
            else:
                cluster_data = clusters[clus]["cluster_data_2wk"]
            cluster_data = cluster_data.sort_index()
            if division:
                total_data = clusters[clus][selected_country]["total_data_2wk_div"]
            else:
                total_data = clusters[clus]["total_data_2wk"]

            if coun not in cluster_data:
                i += 1
                continue
            (
                week_as_date,
                cluster_count,
                total_count,
                unsmoothed_cluster_count,
                unsmoothed_total_count,
            ) = non_zero_counts(cluster_data, total_data, coun)
            
            if len(total_count)<2:
                continue

            # trim away any last data points that only have 1 or 2 seqs
            week_as_date, cluster_count, total_count = trim_last_data_point(
                week_as_date, cluster_count, total_count, frac=0.1, keep_count=10
            )

            mindat = min(week_as_date)
            if mindat < min_week:
                min_week = mindat
            maxdat = max(week_as_date)
            if maxdat > max_week:
                max_week = maxdat

            week_as_dates[coun] = week_as_date

            country_data[clusters[clus]["display_name"]] = list(
                cluster_count
            )

            country_week[clus][coun] = cluster_count / total_count

            linesty = "-"
            lab = clusters[clus]["display_name"]
            if i == 0:
                first_clus_count = [0] * len(total_count)
            if len(first_clus_count) == 0:
                first_clus_count = [0] * len(total_count)
            cluster_count = first_clus_count + cluster_count  # unindented

            first_clus_count = cluster_count  # unindented
            i += 1
        country_data["week"] = [
            datetime.datetime.strftime(x, "%Y-%m-%d") for x in week_as_date
        ]
        country_data["total_sequences"] = [
            int(x) for x in total_count
        ]
        if len(total_count)>=2:
            json_output["countries"][coun] = country_data

    json_output["plotting_dates"] = {}
    json_output["plotting_dates"]["min_date"] = datetime.datetime.strftime(
        min_week, "%Y-%m-%d"
    )
    json_output["plotting_dates"]["max_date"] = datetime.datetime.strftime(
        max_week, "%Y-%m-%d"
    )

    if print_files:
        with open(tables_path + f"{file_prefix}_data.json", "w") as fh:
            json.dump(json_output, fh)


if do_country:
    proposed_coun_to_plot, clus_keys = get_ordered_clusters_to_plot(clusters)
    plot_country_data(
        clusters, proposed_coun_to_plot, print_files, clus_keys, "EUClusters"
    )


if do_divisions_country:
    proposed_coun_to_plot, clus_keys = get_ordered_clusters_to_plot(
        clusters, True, "USA"
    )
    plot_country_data(
        clusters,
        proposed_coun_to_plot,
        print_files,
        clus_keys,
        "USAClusters",
        True,
        "USA",
    )

    proposed_coun_to_plot, clus_keys = get_ordered_clusters_to_plot(
        clusters, True, "Switzerland"
    )
    plot_country_data(
        clusters,
        proposed_coun_to_plot,
        print_files,
        clus_keys,
        "SwissClusters",
        True,
        "Switzerland",
    )

# if all went well (script got to this point), and did an 'all' run, then print out an update!
# from datetime import datetime
update_json = {"lastUpdated": str(datetime.datetime.now().isoformat())}

if print_files and "all" in clus_answer:
    with open(web_data_folder + f"update.json", "w") as fh:
        json.dump(update_json, fh)

if "all" in clus_answer:
    ccounts = []
    for clus in clusters:
        if clusters[clus]['type'] == "variant":
            displayn = clusters[clus]["display_name"]
            ccounts.append([displayn, len(clusters[clus]['cluster_meta'])])

    count_df = pd.DataFrame(ccounts, columns=['cluster', 'counts'])
    print("Showing cluster counts")
    print(count_df.sort_values(by="counts"))

#repeat to be sure they are seen
print("\nDate alerts:")
print([f"{x}: {len(alert_first_date[x])}" for x in alert_first_date.keys()])
print("To view, use 'print_date_alerts(clus)' or 'print_all_date_alerts()'")

#reminder to check for countries without colors or no longer plotted

if "all" in clus_answer:
    no_colors = [coun for coun in countries_plotted.keys() if coun not in country_styles_all]
    if len(no_colors) > 0:
        print(
            f"REMINDER: {len(no_colors)} countries have no color. Scroll up to see them!"
        )
    not_plotted = [coun for coun in country_styles_all if coun not in countries_plotted.keys()]
    if len(not_plotted) > 0:
        print(
            f"REMINDER: {len(not_plotted)} countries are not plotted anymore. Scroll up to see them!"
        )


